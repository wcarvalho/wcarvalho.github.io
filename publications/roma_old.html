---
title: "Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in First-person Simulated 3D Environments"
layout: project_page
tags: [object-oriented-learning, reinforcement-learning, representation-learning]
---

<br><br>
<center>
<div class="container">

  <div class="row">
    <div class="col">
      <a href="/"><span style="color: #9f30a5">Wilka Carvalho</span></a><sup>1</sup>&emsp;
      <a href="https://aliang8.github.io/">Anthony Liang</a><sup>1</sup>&emsp;
      <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a><sup>3</sup>&emsp;
      <a href="https://sites.google.com/view/sungryull">Sungryull Sohn</a><sup>1</sup>
    </div>
  </div>


  <div class="row">
    <div class="col">
      <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a><sup>1,4</sup>&emsp;
      <a href="http://www-personal.umich.edu/~rickl/">Richard L. Lewis</a><sup>2</sup>&emsp;
      <a href="https://web.eecs.umich.edu/~baveja/">Satinder Singh</a><sup>1,5</sup>
    </div>
  </div>

  <div class="row">
    <div class="col">
      <sup>1</sup>University of Michigan, Dept. of Computer Science
    </div>
  </div>

  <div class="row">
    <div class="col">
      <sup>2</sup>University of Michigan, Dept. of Psychology
    </div>
  </div>

  <div class="row">
    <div class="col">
      <sup>3</sup>UC Berkeley&emsp;<sup>4</sup>Google Brain&emsp;<sup>5</sup>DeepMind
    </div>
  </div>

</div>
</center>

<br><br>






<br>
<center>
  <div class="embed-responsive embed-responsive-16by9">
  <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/E8xqlK8cuKw" allowfullscreen></iframe>
  </div>
</center>
<!-- <center><img class="responsive-img" style="max-width: 750" src="/files/publications/roma/architecture.png"></center>
 -->
<br>
<!-- <h1>Abstract</h1> -->
Sequential decision-making tasks that require relating and using multiple novel objects pose significant sample-efficiency challenges for agents learning from sparse task rewards. In this work, we begin to address these challenges by leveraging an agent's object-interactions to define an auxilliary task that enables sample-efficient reinforcement learning (RL) of such tasks. To accomplish this, we formulate ROMA: a relational reinforcement learning agent that learns an object-centric forward model during task learning. We find that this enables it to learn object-interaction tasks much faster than other relational RL agents with alternative auxiliary tasks for driving good object-representation learning.  In order to evaluate the performance of our agent, we introduce a set of object-interaction tasks in the AI2Thor virtual home environment that require relating and interacting with multiple objects.  By comparing against an agent equipped with ground-truth object-information, we find that learning an object-centric forward model best closes the performance gap, achieving $\geq 80\%$ of its sample-efficiency on $7$ out of $8$ tasks, with the next best method doing so on $3$ out of $8$ tasks. Additionally, we find that our object-model best captures interesting object information such as category, specific object state, and relationships among objects.

<br><br>
<div class="row">
<div class="row pub-links">
  <p>
    <!-- <a href="{{ site.baseurl }}/publications/roma"> <button type = "button" class = "btn btn-primary"> Blog </button></a> -->
    <a href="{{ site.baseurl }}/files/publications/roma/roma.pdf"> <button type = "button" class = "btn btn-primary"> Paper </button></a>
    <a href="https://twimlai.com/relational-object-centric-agents-for-completing-simulated-household-tasks-with-wilka-carvalho/">
        <button type = "button" class = "btn btn-primary">
        Podcast
        </button>
    </a>
  </p>
</div>
</div>

<br><br><br><br>
<hr>
<br>
<center>
  <h1>Relational, Object-Model Learning Agent</h1>
</center>
When deciding to interact with an object, our agent uses self-attention to attend to other objects so it can incorporate relationships among objects into its decision making. Afterward, it predicts the consequnces of its object-interaction, factoring in the object relationships it computed via self-attention. This provides a dense learning signal for learning about objects and their relationships prior to experiencing task reward.
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/overview.png">
</center>


<br><br><br>
<hr>
<br>
<center>
  <h1>Object-Centric Decision Making</h1>
</center>
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/architecture.png">
</center>
A scene is broken down into object-image-patches $\{o_j\}$ (e.g. of a pot, potato, and stove knob). The scene image is combined with the agent's location to define the <i>context</i> of the objects, $s^\kappa$. The objects  $\{o_j\}$ and their context $s^\kappa$ are processed by different encoding branches and then recombined by a relational module $\mathcal{R}$ that uses self-attention to produce inputs for computing Q-value estimates. Here, $\mathcal{R}$ might select the pot image-patch when computing Q-values for interacting with the stove-knob image-patch. Actions are selected as (object-image-patch, base action) pairs $a=(o_c, b)$. The agent then predicts the consequences of its interactions with our relational object-model $f_{\tt model}$.




<br><br><br>
<hr>
<br>
<center>
  <h1>Results</h1>
</center>
<h3>A basic object level understanding isn't sufficient for our challenging tasks</h3>
It is well-known that providing an object-level understanding improves performance and sample-efficiency. Despite, below we see that supplying DQN with a basic object-level understanding via object-image-patches or object-ids is insufficient to learn challenging tasks that require reasoning over multiple objects and the object-state they occupy. We hypothesize that this is due to the large-branching factor induced by object-interactions and the sparse-reward learning signal (random policies complete tasks $\approx 10\times$ in 500K samples).
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/ablation.png">
</center>
<h3>The key is combining a relational object-centric policy (Relational Object-DQN) with a Relational Object-Model</h3>
Interestingly, despite facing a sparse-reward problem, we find that with a basic $\epsilon$-greedy exploration strategy, simply adding object-attention to an object-centric DQN (Relational Object-DQN, grey curve in Figure below) enables progress towards efficient learning. Somewhat surprisingly, we find that adding rich hand-designed object-representations (Relational Object-DQN + Ground-Truth Object-Information, black curve in Figure below) enables learning of all the tasks we study. <b>This indicates that we can efficiently learn from only a few rewarding instances with a good object-centric inductive bias</b>. 
<br>
<br>
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/lower-upper.png">
</center>
<br>
We find that adding a strong unsupervised object-representation learning method to our Relational Object-DQN such as OCN or COBRA's non-relational object-model helps but is insufficient. However, adding our <i>Relational</i> Object-Model enables $\geq 75\%$ ground-truth sample-efficiency on $7/8$ tasks. Thus, the key to sample-efficient learning is bootstraping learning of both object-representations <b>and</b> their relations.
<br><br>
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/auc.png">
</center>
<!-- <br>
Additionally, we find that our relational object-model best captures interesting object information such as category, specific object state, and relationships among objects.
<center>
  <img class="responsive-img" style="max-width: 300" src="{{ site.baseurl }}/files/publications/roma/table.png">
</center> -->