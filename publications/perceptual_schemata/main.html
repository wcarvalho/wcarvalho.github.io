---
title: "Learning to Represent State with Perceptual Schemata"
layout: project_page
tags: [reinforcement-learning, representation-learning]
---

<br><br>
<center>
<div class="container">

  <div class="row">
    <div class="col">
      <a href="/"><span style="color: #9f30a5">Wilka Carvalho</span></a><sup>1,2</sup>&emsp;
      <a href="https://www.doc.ic.ac.uk/~mpsha/">Murray Shanahan</a><sup>2</sup>&emsp;
    </div>
  </div>


  <div class="row">
    <div class="col">
      <sup>1</sup>University of Michigan&emsp;<sup>2</sup>DeepMind
    </div>
  </div>

  <div class="row">
    <div class="col">
      <em> ICML {Unsupervised RL, Real Life RL} Workshops, </em> 2021
    </div>
  </div>

</div>
</center>

<br><br>
<center>
  <img class="responsive-img" style="max-height: 500px;" src="{{ site.baseurl }}/publications/perceptual_schemata/architecture.png">
</center>

<!-- <h1>Abstract</h1> -->
The real world is large and complex. It is filled with many objects besides those defined by a task and objects can move with their own interesting dynamics. How should an agent learn to represent state to support efficient learning and generalization in such an environment? In this work, we present a novel memory architecture, Perceptual Schemata, for learning and zero-shot generalization in environments that have many, potentially moving objects. Perceptual Schemata represents state using a combination of schema modules that each learn to attend to and maintain stateful representations of different subspaces of a spatio-temporal tensor describing the agentâ€™s observations. We present empirical results that Perceptual Schemata enables a state representation that can maintain multiple objects observed in sequence with independent dynamics while an LSTM cannot. We additionally show that Perceptual Schemata can generalize more gracefully to larger environments with more distractor objects, while an LSTM quickly overfits to the training tasks.



